{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86fd9846",
   "metadata": {},
   "source": [
    "#### Conceptos de este cuaderno :\n",
    "\n",
    "## Con que trabaja spark?\n",
    "\n",
    "##### DataSet(Datos para explotar, normalmente en formato CSV, JSON o similares). Los 'DataFrames'(No distribuido, mutable y no tolerante a fallos) y los 'RDD(Resilient Distributed Dataset)'(Distribuidos, inmutable y tolerante a fallos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a7dd8",
   "metadata": {},
   "source": [
    "#### En estos primeras celdas trabajaremos con Apache Spark en modo 'Standalone', es decir, que tendremos todo corriendo en una sola máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb578f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Que version de Pyspark estamos usando? \n",
    "\n",
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650754b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Que version de Python estamos usando? \n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90061cc3",
   "metadata": {},
   "source": [
    "### Optimizacion de recursos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85dedc",
   "metadata": {},
   "source": [
    "##### PySpark utiliza memoria tanto para el almacenamiento en caché como para el procesamiento. Puedes configurar la cantidad de memoria que PySpark debe utilizar para cada uno de estos propósitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99dc0cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configuración de memoria\n",
    "from pyspark import SparkConf\n",
    "conf = SparkConf().setAppName(\"EcommerceCosmeticShop\").set(\"spark.driver.memory\", \"4g\").set(\"spark.executor.memory\", \"4g\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca852665",
   "metadata": {},
   "source": [
    "#### Paralelismo y número de ejecutores: Otra configuración importante es el número de ejecutores y el nivel de paralelismo. Esto afecta cómo se divide la carga de trabajo. Puedes configurar la cantidad de ejecutores y los hilos de cada executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffe351b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fb99f4a1c10>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Configuracion de Paralelismo y número de ejecutores \n",
    "conf.set(\"spark.executor.instances\", \"4\").set(\"spark.executor.cores\", \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "835a7c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear un dataframe de cliente declarando el esquema y pasando valores\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3618dfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"EcommerceCosmeticShop\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052a1d17",
   "metadata": {},
   "source": [
    "# terminal \n",
    "#Arrancamos el master en modo standalone(Arranca WebUI en 127.0.0.1:8080)\n",
    "$SPARK_HOME/sbin/start-master.sh\n",
    "\n",
    " Arrancamos el primer esclavo\n",
    "$SPARK_HOME/sbin/start-slave.sh spark://MacBook-Pro-de-Lucas-880.local:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2274c510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Leemos datos del DataSet y los escribimos en un DataFrame\n",
    "df = spark.read.options(header='True', inferSchema='True').csv(['2019-Dec.csv', '2019-Nov.csv', '2019-Oct.csv', '2020-Feb.csv', '2020-Jan.csv'])\n",
    "# Escribimos el DataFrame en disco en la carpeta donde se encuentra este Jupyter Notebook\n",
    "df.write.mode('overwrite').csv(\"ecommerce-cosmetic-shop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1399c07",
   "metadata": {},
   "source": [
    "##### Detener la sesión de Spark cuando hayas terminado\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd08df0",
   "metadata": {},
   "source": [
    "Comprobar que el proceso ha generado ficheros CSVs. El motivo es que Apache Spark particiona los datos para procesar en paralelo y, por lo tanto, crea un fichero por partición(También ha creado un fichero '_SUCCESS' para indicar el momento en que había acabado de exportar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f12801a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20692840"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Contar número de registros de un DataFrame\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2ca8ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Escribir el Schema de los datos del Data Frame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f34240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=====================================================>  (18 + 1) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|      event_type|\n",
      "+----------------+\n",
      "|        purchase|\n",
      "|            view|\n",
      "|            cart|\n",
      "|remove_from_cart|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Mostrar o contar los diferentes valores de una columna\n",
    "df.select('event_type').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98e84e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54571"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar o contar los diferentes valores de una columna\n",
    "df.select('product_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8589200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:=====================================================>  (18 + 1) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|       brand|\n",
      "+------------+\n",
      "|     beautix|\n",
      "|     farmona|\n",
      "|  dr.gloderm|\n",
      "|   profhenna|\n",
      "|     philips|\n",
      "|invisibobble|\n",
      "|       riche|\n",
      "|        oniq|\n",
      "|    lebelage|\n",
      "|     vilenta|\n",
      "|       fancy|\n",
      "|      jaguar|\n",
      "|      tertio|\n",
      "|    siberina|\n",
      "|   koreatida|\n",
      "|         jas|\n",
      "|rocknailstar|\n",
      "|   depilflax|\n",
      "|protokeratin|\n",
      "|       essie|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select('brand').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fceaeac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(product_id=4958)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener el primer 'product_id' del registro con 'event_type=cart'\n",
    "df.select(['product_id']).filter(\"event_type='cart'\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bab51c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los productos que se han comprado conjuntamente con el anterior registro\n",
    "sesions=df.select(['user_session']).filter(\"event_type='cart' AND product_id=5844305\").distinct().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf748e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|product_id|\n",
      "+----------+\n",
      "|   5820774|\n",
      "|   5820776|\n",
      "|   5820777|\n",
      "|   5870111|\n",
      "|   5844303|\n",
      "|   5820721|\n",
      "|   5698887|\n",
      "|   5698879|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "products=df.select(['product_id']).filter(\"event_type='cart' AND product_id<>5844305\").filter( df[\"user_session\"].isin(sesions[\"user_session\"]))\n",
    "products.select('product_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0583c9b5",
   "metadata": {},
   "source": [
    "### SQL API - opcion b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dad266a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear un objeto SparkSession\n",
    "spark = SparkSession.builder.appName(\"EcommerceCosmeticShop\").getOrCreate()\n",
    "\n",
    "# Crear una vista temporal\n",
    "df.createOrReplaceTempView(\"datasql\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a408b75d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o24.sql. Trace:\npy4j.Py4JException: Method sql([class java.lang.String, class [Ljava.lang.Object;]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ejecutar una consulta SQL y mostrar los resultados\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM datasql LIMIT 5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/anaconda3/envs/ecomspy/lib/python3.9/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ecomspy/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/ecomspy/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/envs/ecomspy/lib/python3.9/site-packages/py4j/protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o24.sql. Trace:\npy4j.Py4JException: Method sql([class java.lang.String, class [Ljava.lang.Object;]) does not exist\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)\n\tat py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)\n\tat py4j.Gateway.invoke(Gateway.java:274)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar una consulta SQL y mostrar los resultados\n",
    "spark.sql(\"SELECT * FROM datasql LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35239d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFunc(s):\n",
    "  if s[\"brand\"]==\"riche\" and s[\"event_type\"]==\"cart\":\n",
    "    return [ ( s[\"product_id\"], 1) ]\n",
    "  return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=df.rdd.flatMap(myFunc).reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb4eca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in lines.collect(): \n",
    "  print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d8b71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
