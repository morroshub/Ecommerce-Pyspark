{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86fd9846",
   "metadata": {},
   "source": [
    "#### Conceptos de este cuaderno :\n",
    "\n",
    "## Con que trabaja spark?\n",
    "\n",
    "##### DataSet(Datos para explotar, normalmente en formato CSV, JSON o similares). Los 'DataFrames'(No distribuido, mutable y no tolerante a fallos) y los 'RDD(Resilient Distributed Dataset)'(Distribuidos, inmutable y tolerante a fallos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a7dd8",
   "metadata": {},
   "source": [
    "#### En estos primeras celdas trabajaremos con Apache Spark en modo 'Standalone', es decir, que tendremos todo corriendo en una sola máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecb578f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.1\n"
     ]
    }
   ],
   "source": [
    "# Que version de Pyspark estamos usando? \n",
    "\n",
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "650754b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.13 (main, Aug 25 2022, 18:29:29) \n",
      "[Clang 12.0.0 ]\n"
     ]
    }
   ],
   "source": [
    "# Que version de Python estamos usando? \n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "835a7c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear un dataframe de cliente declarando el esquema y pasando valores\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3618dfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"EcommerceCosmeticShop\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052a1d17",
   "metadata": {},
   "source": [
    "# terminal \n",
    "#Arrancamos el master en modo standalone(Arranca WebUI en 127.0.0.1:8080)\n",
    "$SPARK_HOME/sbin/start-master.sh\n",
    "\n",
    " Arrancamos el primer esclavo\n",
    "$SPARK_HOME/sbin/start-slave.sh spark://MacBook-Pro-de-Lucas-880.local:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2274c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos datos del DataSet y los escribimos en un DataFrame\n",
    "df = spark.read.options(header='True', inferSchema='True').csv(['2019-Dec.csv', '2019-Nov.csv', '2019-Oct.csv', '2020-Feb.csv', '2020-Jan.csv'])\n",
    "# Escribimos el DataFrame en disco en la carpeta donde se encuentra este Jupyter Notebook\n",
    "df.write.mode('overwrite').csv(\"ecommerce-cosmetic-shop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1399c07",
   "metadata": {},
   "source": [
    "##### Detener la sesión de Spark cuando hayas terminado\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd08df0",
   "metadata": {},
   "source": [
    "Comprobar que el proceso ha generado ficheros CSVs. El motivo es que Apache Spark particiona los datos para procesar en paralelo y, por lo tanto, crea un fichero por partición(También ha creado un fichero '_SUCCESS' para indicar el momento en que había acabado de exportar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12801a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar número de registros de un DataFrame\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ca8ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribir el Schema de los datos del Data Frame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f34240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar o contar los diferentes valores de una columna\n",
    "df.select('event_type').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e84e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar o contar los diferentes valores de una columna\n",
    "df.select('product_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8589200",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('brand').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceaeac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el primer 'product_id' del registro con 'event_type=cart'\n",
    "df.select(['product_id']).filter(\"event_type='cart'\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bab51c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los productos que se han comprado conjuntamente con el anterior registro\n",
    "sesions=df.select(['user_session']).filter(\"event_type='cart' AND product_id=5844305\").distinct().first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf748e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "products=df.select(['product_id']).filter(\"event_type='cart' AND product_id<>5844305\").filter( df[\"user_session\"].isin(sesions[\"user_session\"]))\n",
    "products.select('product_id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0583c9b5",
   "metadata": {},
   "source": [
    "### SQL API - opcion b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a408b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"data\")\n",
    "spark.sql(\"select * from data limit 5\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35239d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFunc(s):\n",
    "  if s[\"brand\"]==\"riche\" and s[\"event_type\"]==\"cart\":\n",
    "    return [ ( s[\"product_id\"], 1) ]\n",
    "  return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bd182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines=df.rdd.flatMap(myFunc).reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb4eca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in lines.collect(): \n",
    "  print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d8b71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
